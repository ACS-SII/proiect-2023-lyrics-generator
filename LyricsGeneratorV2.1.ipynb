{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b729892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import torch as torch\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "import torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64665099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f767004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Heshy/.cache/huggingface/datasets/csv/default-004d8cf9f45e9b24/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47ab65ec0a74fc981241c83747b4a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files = 'spotify_millsongdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1182343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 50258. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "\n",
    "model = GPT2Model.from_pretrained('gpt2-medium')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = '[PAD]'\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0050cfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[17694,   375,   494,  2429,   450,  7012]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"melodie gen abba\", return_tensors=\"pt\")\n",
    "encoded_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "278d5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 6\n",
    "tokenized_data = tokenizer(\n",
    "    dataset['train']['text'],\n",
    "    truncation = True,\n",
    "    max_length = MAX_LENGTH,\n",
    "    return_overflowing_tokens = True,\n",
    "    return_length = True\n",
    ")\n",
    "tokenized_data = {\"input_ids\": tokenized_data[\"input_ids\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6df7feb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Heshy\\.cache\\huggingface\\datasets\\csv\\default-004d8cf9f45e9b24\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-b312978bb9f2b727.arrow\n"
     ]
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    try:\n",
    "        outputs = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "        padding=\"max_length\",  # Aceasta va utiliza pad_token implicit\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "        \n",
    "        input_batch = outputs[\"input_ids\"].tolist()\n",
    "\n",
    "        if len(input_batch) < BATCH_SIZE:\n",
    "            input_batch += [input_batch[0]] * (BATCH_SIZE - len(input_batch))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        input_batch = []\n",
    "\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, remove_columns = dataset['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc2b9727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 57650\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c683609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Heshy\\.cache\\huggingface\\datasets\\csv\\default-004d8cf9f45e9b24\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-65a72645aea9cc86.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 57650\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_none(example):\n",
    "    if len(example['input_ids']) >= 1:\n",
    "        return example\n",
    "final_data = tokenized_dataset.filter(filter_none)\n",
    "final_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c087b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_col = DataCollatorForLanguageModeling(tokenizer, mlm = False, return_tensors = \"tf\") # mlm = masked language modeling\n",
    "\n",
    "tf_train_dataset = final_data[\"train\"].to_tf_dataset(\n",
    "    columns = ['input_ids', 'attention_mask', 'labels'],\n",
    "    shuffle = True,\n",
    "    batch_size = 1,\n",
    "    collate_fn = data_col\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75ac32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def swap_position(example):\n",
    "    return {\n",
    "        'input_ids': example['input_ids'],\n",
    "        'attention_mask': tf.ones([1, BATCH_SIZE, MAX_LENGTH]),\n",
    "        'labels': example['labels']\n",
    "    }\n",
    "train_data = tf_train_dataset.map(swap_position)\n",
    "unbatched_train_data = train_data.unbatch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7881228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLay  multiple                  354823168 \n",
      " er)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354823168 (1.32 GB)\n",
      "Trainable params: 354823168 (1.32 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel\n",
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcb657fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "num_train_steps = len(unbatched_train_data)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr = 5e-5,\n",
    "    num_warmup_steps = 1_000,\n",
    "    num_train_steps = num_train_steps\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3284559e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'tfgpt2lm_head_model/transformer/assert_less/Assert/Assert' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Heshy\\AppData\\Local\\Temp\\ipykernel_35580\\926201961.py\", line 1, in <module>\n      train_history = model.fit(unbatched_train_data, epochs = 5)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1637, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 805, in run_call_with_unpacked_inputs\n      else:\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 837, in call\n      transformer_outputs = self.transformer(\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 805, in run_call_with_unpacked_inputs\n      else:\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 437, in call\n      if inputs_embeds is None:\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 438, in call\n      check_embeddings_within_bounds(input_ids, self.config.vocab_size)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\transformers\\tf_utils.py\", line 161, in check_embeddings_within_bounds\n      tf.debugging.assert_less(\nNode: 'tfgpt2lm_head_model/transformer/assert_less/Assert/Assert'\nassertion failed: [The maximum value of input_ids (Tensor(\\\"tfgpt2lm_head_model/transformer/Max:0\\\", shape=(), dtype=int32)) must be smaller than the embedding layer\\'s input dimension (50257). The likely cause is some problem at tokenization time.] [Condition x < y did not hold element-wise:] [x (tfgpt2lm_head_model/transformer/Reshape:0) = ] [[1639 765 262...]...] [y (tfgpt2lm_head_model/transformer/Cast/x:0) = ] [50257]\n\t [[{{node tfgpt2lm_head_model/transformer/assert_less/Assert/Assert}}]] [Op:__inference_train_function_80176]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(unbatched_train_data, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'tfgpt2lm_head_model/transformer/assert_less/Assert/Assert' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Heshy\\AppData\\Local\\Temp\\ipykernel_35580\\926201961.py\", line 1, in <module>\n      train_history = model.fit(unbatched_train_data, epochs = 5)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1637, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 805, in run_call_with_unpacked_inputs\n      else:\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 837, in call\n      transformer_outputs = self.transformer(\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 805, in run_call_with_unpacked_inputs\n      else:\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 437, in call\n      if inputs_embeds is None:\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 438, in call\n      check_embeddings_within_bounds(input_ids, self.config.vocab_size)\n    File \"C:\\Users\\Heshy\\anaconda3\\Lib\\site-packages\\transformers\\tf_utils.py\", line 161, in check_embeddings_within_bounds\n      tf.debugging.assert_less(\nNode: 'tfgpt2lm_head_model/transformer/assert_less/Assert/Assert'\nassertion failed: [The maximum value of input_ids (Tensor(\\\"tfgpt2lm_head_model/transformer/Max:0\\\", shape=(), dtype=int32)) must be smaller than the embedding layer\\'s input dimension (50257). The likely cause is some problem at tokenization time.] [Condition x < y did not hold element-wise:] [x (tfgpt2lm_head_model/transformer/Reshape:0) = ] [[1639 765 262...]...] [y (tfgpt2lm_head_model/transformer/Cast/x:0) = ] [50257]\n\t [[{{node tfgpt2lm_head_model/transformer/assert_less/Assert/Assert}}]] [Op:__inference_train_function_80176]"
     ]
    }
   ],
   "source": [
    "train_history = model.fit(unbatched_train_data, epochs = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d5aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28fe09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8cfe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6003bd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc27ce04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
